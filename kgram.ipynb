{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498af377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/rohithk/miniconda3/envs/blackbox-model-tracing/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-28 11:29:11 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:29:11,868\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, LlamaConfig\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import vllm\n",
    "from vllm import LLM\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355588ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate(tokens,kgram_index,k_max,shuffle_order):\n",
    "    shuffled_idx = np.argsort(shuffle_order)\n",
    "    results = []\n",
    "    for pos in range(len(tokens)-k_max):\n",
    "        k = k_max\n",
    "        while k > 0:\n",
    "            prefix = tokens[pos:pos+k]\n",
    "            if prefix in kgram_index:\n",
    "                actual_next_token = tokens[pos+k]\n",
    "                shuffled_idx_avg = 0\n",
    "                num_matches = 0\n",
    "                for info in kgram_index[prefix]:\n",
    "                    if info['next_token'] == actual_next_token:\n",
    "                        num_matches += 1\n",
    "                        shuffled_idx_avg += shuffled_idx[info['idx']]\n",
    "                        \n",
    "                if num_matches > 0:\n",
    "                    shuffled_idx_avg /= num_matches\n",
    "                    results.append(shuffled_idx_avg)\n",
    "                    break\n",
    "            k -= 1\n",
    "                    \n",
    "    return results\n",
    "\n",
    "def generate_and_correlate_text(prompts,llm,kgram_index,k,shuffle_order):\n",
    "    generated = llm.generate(prompts)\n",
    "    full_results = []\n",
    "    for i in range(len(generated)):\n",
    "        tokens = generated[i].outputs[0].token_ids\n",
    "        results = correlate(tokens,kgram_index,k,shuffle_order)\n",
    "        \n",
    "        full_results += results\n",
    "        \n",
    "    return full_results\n",
    "\n",
    "def eval_tiny(model_path, eval_texts):\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    result = perplexity.compute(model_id=model_path,\n",
    "                                add_start_token=True,\n",
    "                                predictions=eval_texts)\n",
    "    pplx = np.log(result['perplexities'])\n",
    "\n",
    "    return pplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618cff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 1\n",
    "control_id = 0\n",
    "model_epoch = 9\n",
    "\n",
    "MODEL_HASH = 'e4c9c1ac'\n",
    "df_path = f'./kgram-analysis/models/tiny_ref_model_{MODEL_HASH}/tinystories.csv'\n",
    "model_path = f'./kgram-analysis/models/tiny_ref_model_{MODEL_HASH}/epoch-{model_epoch}-index-{model_id}'\n",
    "control_path = f'./kgram-analysis/models/tiny_ref_model_{MODEL_HASH}/epoch-{model_epoch}-index-{control_id}'\n",
    "\n",
    "INDEX_HASH = '6f53bb4b'\n",
    "k = 8\n",
    "index_path = f'./kgram-analysis/indexes/kgram_index_{INDEX_HASH}/kgram_index_k{k}.pkl'\n",
    "\n",
    "N_TRAIN_SAMPLES = 10000\n",
    "PROMPT_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d84d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "texts = dataset[\"train\"][\"text\"][:N_TRAIN_SAMPLES]\n",
    "texts = [item for item in texts if item != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b292e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 625/625 [01:26<00:00,  7.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 625/625 [01:26<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SignificanceResult(statistic=-0.36053098296530983, pvalue=1.108995454405034e-304)\n"
     ]
    }
   ],
   "source": [
    "order_epoch = 0\n",
    "\n",
    "model_pplx = eval_tiny(model_path,texts)\n",
    "ctrl_pplx = eval_tiny(control_path,texts)\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "stat = scipy.stats.spearmanr(np.argsort(df[f'order-{model_id}-epoch-{order_epoch}']), model_pplx-ctrl_pplx)\n",
    "\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08275419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-28 11:32:46 [config.py:2610] Downcasting torch.float32 to torch.float16.\n",
      "WARNING 03-28 11:32:57 [arg_utils.py:1854] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 03-28 11:32:57 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='./kgram-analysis/models/tiny_ref_model_e4c9c1ac/epoch-9-index-1', speculative_config=None, tokenizer='./kgram-analysis/models/tiny_ref_model_e4c9c1ac/epoch-9-index-1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=./kgram-analysis/models/tiny_ref_model_e4c9c1ac/epoch-9-index-1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-28 11:32:58 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-28 11:32:58 [cuda.py:288] Using XFormers backend.\n",
      "INFO 03-28 11:32:58 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-28 11:32:58 [model_runner.py:1110] Starting to load model ./kgram-analysis/models/tiny_ref_model_e4c9c1ac/epoch-9-index-1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-28 11:32:59 [loader.py:447] Loading weights took 0.07 seconds\n",
      "INFO 03-28 11:32:59 [model_runner.py:1146] Model loading took 0.0362 GB and 0.106923 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-28 11:32:59 [worker.py:267] Memory profiling takes 0.34 seconds\n",
      "INFO 03-28 11:32:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.28GiB\n",
      "INFO 03-28 11:32:59 [worker.py:267] model weights take 0.04GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.29GiB; the rest of the memory reserved for KV Cache is 20.95GiB.\n",
      "INFO 03-28 11:33:00 [executor_base.py:111] # cuda blocks: 343232, # CPU blocks: 65536\n",
      "INFO 03-28 11:33:00 [executor_base.py:116] Maximum concurrency for 512 tokens per request: 10726.00x\n",
      "INFO 03-28 11:33:02 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|█████████████████████████████████████| 35/35 [00:16<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-28 11:33:18 [model_runner.py:1570] Graph capturing finished in 17 secs, took 0.15 GiB\n",
      "INFO 03-28 11:33:18 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 19.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_path, task=\"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dd70697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rep: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:13<00:00, 746.03it/s, est. speed input: 5068.38 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574\n",
      "rep: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:13<00:00, 746.98it/s, est. speed input: 5074.83 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382\n",
      "rep: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:15<00:00, 648.63it/s, est. speed input: 4406.64 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1478\n",
      "rep: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:13<00:00, 746.08it/s, est. speed input: 5068.69 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442\n",
      "rep: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:15<00:00, 646.87it/s, est. speed input: 4394.70 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1438\n",
      "rep: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:15<00:00, 648.57it/s, est. speed input: 4406.19 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437\n",
      "rep: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:13<00:00, 746.41it/s, est. speed input: 5070.95 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1462\n",
      "rep: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:15<00:00, 646.20it/s, est. speed input: 4390.11 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n",
      "rep: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:13<00:00, 745.38it/s, est. speed input: 5063.96 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371\n",
      "rep: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10000/10000 [00:13<00:00, 744.66it/s, est. speed input: 5059.02 toks/s, outp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503\n",
      "5103.451662218861\n"
     ]
    }
   ],
   "source": [
    "kgram_index = pickle.load(open(index_path,'rb'))\n",
    "shuffle_order = df[f'order-{model_id}-epoch-{order_epoch}']\n",
    "prompts = [text[:PROMPT_LEN] for text in texts]\n",
    "\n",
    "n_reps = 10\n",
    "avg_idx = 0\n",
    "for rep in range(n_reps):\n",
    "    print(f\"rep: {rep}\")\n",
    "    results = generate_and_correlate_text(prompts,llm,kgram_index,k,shuffle_order)\n",
    "    print(len(results))\n",
    "    avg_idx += np.mean(results)/n_reps\n",
    "\n",
    "print(avg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8064583a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4980.459436988101"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec951793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5002.20307263202"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.random.uniform(10000,size=(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75087202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:blackbox-model-tracing]",
   "language": "python",
   "name": "conda-env-blackbox-model-tracing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
