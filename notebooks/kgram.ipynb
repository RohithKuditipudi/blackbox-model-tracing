{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498af377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/rohithk/miniconda3/envs/blackbox-model-tracing/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 02:31:25 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 02:31:25,559\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, LlamaConfig\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355588ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate(tokens,kgram_index,k_max,shuffle_order):\n",
    "    shuffled_idx = np.argsort(shuffle_order)\n",
    "    results = []\n",
    "    for pos in range(len(tokens)-k_max):\n",
    "        k = k_max\n",
    "        while k > 0:\n",
    "            prefix = tuple(tokens[pos:pos+k])\n",
    "            if prefix in kgram_index:\n",
    "                actual_next_token = tokens[pos+k]\n",
    "                shuffled_idx_avg = 0\n",
    "                num_matches = 0\n",
    "                for info in kgram_index[prefix]:\n",
    "                    if info['next_token'] == actual_next_token:\n",
    "                        num_matches += 1\n",
    "                        shuffled_idx_avg += shuffled_idx[info['idx']]\n",
    "                        \n",
    "                if num_matches > 0:\n",
    "                    shuffled_idx_avg /= num_matches\n",
    "                    results.append(shuffled_idx_avg)\n",
    "                    break\n",
    "            k -= 1\n",
    "                    \n",
    "    return results\n",
    "\n",
    "def generate_and_correlate_text(prompts,llm,kgram_index,k,shuffle_order,sampling_params):\n",
    "    generated = llm.generate(prompts,sampling_params)\n",
    "    full_results = []\n",
    "    for i in range(len(generated)):\n",
    "        tokens = generated[i].outputs[0].token_ids\n",
    "        results = correlate(tokens,kgram_index,k,shuffle_order)\n",
    "        \n",
    "        full_results += results\n",
    "        \n",
    "    return full_results\n",
    "\n",
    "def eval_tiny(model_path, eval_texts):\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    result = perplexity.compute(model_id=model_path,\n",
    "                                add_start_token=True,\n",
    "                                predictions=eval_texts)\n",
    "    pplx = np.log(result['perplexities'])\n",
    "\n",
    "    return pplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618cff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 1\n",
    "control_id = 0\n",
    "model_epoch = 9\n",
    "\n",
    "MODEL_HASH = 'e4c9c1ac'\n",
    "df_path = f'./kgram-analysis/models/tiny_ref_model_{MODEL_HASH}/tinystories.csv'\n",
    "model_path = f'./kgram-analysis/models/tiny_ref_model_{MODEL_HASH}/epoch-{model_epoch}-index-{model_id}'\n",
    "control_path = f'./kgram-analysis/models/tiny_ref_model_{MODEL_HASH}/epoch-{model_epoch}-index-{control_id}'\n",
    "\n",
    "INDEX_HASH = '9f87af1f'\n",
    "k = 20\n",
    "index_path = f'./kgram-analysis/indexes/kgram_index_{INDEX_HASH}/kgram_index_k{k}.pkl'\n",
    "\n",
    "N_TRAIN_SAMPLES = 10000\n",
    "PROMPT_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d84d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "texts = dataset[\"train\"][\"text\"][:N_TRAIN_SAMPLES]\n",
    "texts = [item for item in texts if item != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4efbb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = f'./kgram-analysis/models/tiny_ref_model_{MODEL_HASH}/tinystories.csv'\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b292e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/625 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.33 GiB. GPU 0 has a total capacity of 47.54 GiB of which 1.59 GiB is free. Process 3468247 has 43.02 GiB memory in use. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 2.60 GiB is allocated by PyTorch, and 14.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m order_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model_pplx \u001b[38;5;241m=\u001b[39m \u001b[43meval_tiny\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ctrl_pplx \u001b[38;5;241m=\u001b[39m eval_tiny(control_path,texts)\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(df_path)\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36meval_tiny\u001b[0;34m(model_path, eval_texts)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval_tiny\u001b[39m(model_path, eval_texts):\n\u001b[1;32m     37\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mperplexity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                \u001b[49m\u001b[43madd_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     pplx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperplexities\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pplx\n",
      "File \u001b[0;32m/nlp/scr/rohithk/miniconda3/envs/blackbox-model-tracing/lib/python3.10/site-packages/evaluate/module.py:467\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {input_name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed):\n\u001b[0;32m--> 467\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/juice4/scr4/nlp/model-tracing/modules/evaluate_modules/metrics/evaluate-metric--perplexity/8ab643ad86f568b7d1d5f7822373fa7401ff5ff0297ccf114b0ca6a33be96bc0/perplexity.py:181\u001b[0m, in \u001b[0;36mPerplexity._compute\u001b[0;34m(self, predictions, model_id, batch_size, add_start_token, device, max_length)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    179\u001b[0m     out_logits \u001b[38;5;241m=\u001b[39m model(encoded_batch, attention_mask\u001b[38;5;241m=\u001b[39mattn_mask)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m--> 181\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m \u001b[43mout_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    183\u001b[0m shift_attention_mask_batch \u001b[38;5;241m=\u001b[39m attn_mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.33 GiB. GPU 0 has a total capacity of 47.54 GiB of which 1.59 GiB is free. Process 3468247 has 43.02 GiB memory in use. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 2.60 GiB is allocated by PyTorch, and 14.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "order_epoch = 0\n",
    "\n",
    "model_pplx = eval_tiny(model_path,texts)\n",
    "ctrl_pplx = eval_tiny(control_path,texts)\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "stat = scipy.stats.spearmanr(np.argsort(df[f'order-{model_id}-epoch-{order_epoch}']), model_pplx-ctrl_pplx)\n",
    "\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08275419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 02:31:44 [config.py:2610] Downcasting torch.float32 to torch.float16.\n",
      "INFO 04-04 02:31:51 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-04 02:31:52 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='./kgram-analysis/models/tiny_ref_model_e4c9c1ac/epoch-9-index-1', speculative_config=None, tokenizer='./kgram-analysis/models/tiny_ref_model_e4c9c1ac/epoch-9-index-1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=./kgram-analysis/models/tiny_ref_model_e4c9c1ac/epoch-9-index-1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-04 02:31:53 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x151e63bf7910>\n",
      "INFO 04-04 02:31:53 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-04 02:31:53 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-04 02:31:53 [gpu_model_runner.py:1174] Starting to load model ./kgram-analysis/models/tiny_ref_model_e4c9c1ac/epoch-9-index-1...\n",
      "WARNING 04-04 02:31:54 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 49.20it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 02:31:54 [loader.py:447] Loading weights took 0.03 seconds\n",
      "INFO 04-04 02:31:54 [gpu_model_runner.py:1186] Model loading took 0.0362 GB and 0.228102 seconds\n",
      "INFO 04-04 02:31:56 [backends.py:415] Using cache directory: /sailhome/rohithk/.cache/vllm/torch_compile_cache/a916b86176/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-04 02:31:56 [backends.py:425] Dynamo bytecode transform time: 2.49 s\n",
      "INFO 04-04 02:31:57 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 04-04 02:31:58 [monitor.py:33] torch.compile takes 2.49 s in total\n",
      "INFO 04-04 02:31:59 [kv_cache_utils.py:566] GPU KV cache size: 10,810,144 tokens\n",
      "INFO 04-04 02:31:59 [kv_cache_utils.py:569] Maximum concurrency for 512 tokens per request: 21113.56x\n",
      "INFO 04-04 02:32:16 [gpu_model_runner.py:1534] Graph capturing finished in 17 secs, took 0.24 GiB\n",
      "INFO 04-04 02:32:16 [core.py:151] init engine (profile, create kv cache, warmup model) took 22.53 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_path, task=\"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec951793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5002.20307263202"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.random.uniform(10000,size=(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75087202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rep: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 1188.68it/s, est. speed input: 7914.52 toks/s, output:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2611\n",
      "rep: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 788.75it/s, est. speed input: 5249.55 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2559\n",
      "rep: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 802.82it/s, est. speed input: 5343.09 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "rep: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 796.73it/s, est. speed input: 5302.67 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2548\n",
      "rep: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 796.74it/s, est. speed input: 5302.62 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2582\n",
      "rep: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 772.71it/s, est. speed input: 5142.81 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600\n",
      "rep: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 789.25it/s, est. speed input: 5252.82 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2569\n",
      "rep: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 790.48it/s, est. speed input: 5259.64 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2572\n",
      "rep: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 790.96it/s, est. speed input: 5264.29 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596\n",
      "rep: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 100/100 [00:00<00:00, 794.94it/s, est. speed input: 5290.62 toks/s, output: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602\n",
      "5049.175325085427\n"
     ]
    }
   ],
   "source": [
    "# kgram_index = pickle.load(open(index_path,'rb'))\n",
    "\n",
    "input_texts = dataset[\"train\"][\"text\"]\n",
    "input_texts = [item for item in input_texts if item != \"\"]\n",
    "\n",
    "shuffle_order = df[f'order-{model_id}-epoch-{order_epoch}']\n",
    "prompts = [text[:PROMPT_LEN] for text in input_texts[10000:10000+100]]\n",
    "sampling_params = SamplingParams(max_tokens=50)\n",
    "\n",
    "n_reps = 10\n",
    "avg_idx = 0\n",
    "for rep in range(n_reps):\n",
    "    print(f\"rep: {rep}\")\n",
    "    results = generate_and_correlate_text(prompts,llm,kgram_index,k,shuffle_order,sampling_params)\n",
    "    print(len(results))\n",
    "    avg_idx += np.mean(results)/n_reps\n",
    "\n",
    "print(avg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "695792e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df626963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts[100:100+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c495e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:blackbox-model-tracing]",
   "language": "python",
   "name": "conda-env-blackbox-model-tracing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
